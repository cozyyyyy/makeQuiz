{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyPej/6msmBTo50D3ta0gefB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"bkjxfTCRhVqr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706983766153,"user_tz":-540,"elapsed":24355,"user":{"displayName":"ryo kojima","userId":"17387270176201379411"}},"outputId":"efc7931b-1c6c-4805-d281-d84940fa2a5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/卒業制作/QAIZ/huggingface_transformers_QAIZ\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!mkdir -p '/content/drive/My Drive/卒業制作/QAIZ/huggingface_transformers_QAIZ/'\n","%cd '/content/drive/My Drive/卒業制作/QAIZ/huggingface_transformers_QAIZ/'"]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers\n","%cd transformers"],"metadata":{"id":"DSxedFtPicnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r ./examples/pytorch/summarization/requirements.txt\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install datasets"],"metadata":{"id":"e9gsqg_8iBXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# ファインチューニングの実行\n","!python ./examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path=sonoisa/t5-base-japanese \\\n","    --do_train \\\n","    --do_eval \\\n","    --train_file=summary_train.csv \\\n","    --validation_file=summary_val.csv \\\n","    --num_train_epochs=10 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --save_steps=5000 \\\n","    --save_total_limit=3 \\\n","    --output_dir=summary_ja/ \\\n","    --predict_with_generate \\\n","    --use_fast_tokenizer=False \\\n","    --logging_steps=100"],"metadata":{"id":"ez0ZKzES8ySf","executionInfo":{"status":"ok","timestamp":1699582343260,"user_tz":-540,"elapsed":460737,"user":{"displayName":"ryo kojima","userId":"17387270176201379411"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"20ff1d8c-376a-4c26-f1ce-964ce160bbf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-11-10 02:04:49.123347: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-10 02:04:49.123391: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-10 02:04:49.123426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-11-10 02:04:50.248016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","11/10/2023 02:04:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/10/2023 02:04:54 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=summary_ja/runs/Nov10_02-04-54_0ba6be2df61e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=10.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=summary_ja/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=summary_ja/,\n","save_on_each_node=False,\n","save_safetensors=True,\n","save_steps=5000,\n","save_strategy=steps,\n","save_total_limit=3,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-99366a07f8563659\n","11/10/2023 02:04:55 - INFO - datasets.builder - Using custom data configuration default-99366a07f8563659\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n","11/10/2023 02:04:55 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n","Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n","11/10/2023 02:04:55 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n","Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n","11/10/2023 02:04:55 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n","Downloading data files: 100% 2/2 [00:00<00:00, 4946.11it/s]\n","Downloading took 0.0 min\n","11/10/2023 02:04:55 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","11/10/2023 02:04:55 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:02<00:00,  1.07s/it]\n","Generating train split\n","11/10/2023 02:04:57 - INFO - datasets.builder - Generating train split\n","Generating train split: 356 examples [00:00, 936.47 examples/s]\n","Generating validation split\n","11/10/2023 02:04:57 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 89 examples [00:00, 340.85 examples/s]\n","Unable to verify splits sizes.\n","11/10/2023 02:04:57 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n","11/10/2023 02:04:57 - INFO - datasets.builder - Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n","Downloading (…)lve/main/config.json: 100% 710/710 [00:00<00:00, 3.46MB/s]\n","[INFO|configuration_utils.py:717] 2023-11-10 02:04:59,030 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/config.json\n","[INFO|configuration_utils.py:777] 2023-11-10 02:04:59,035 >> Model config T5Config {\n","  \"_name_or_path\": \"sonoisa/t5-base-japanese\",\n","  \"architectures\": [\n","    \"T5Model\"\n","  ],\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"eos_token_ids\": [\n","    1\n","  ],\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_length\": 512,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_beams\": 4,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"transformers_version\": \"4.36.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","Downloading (…)okenizer_config.json: 100% 1.96k/1.96k [00:00<00:00, 11.8MB/s]\n","[INFO|configuration_utils.py:717] 2023-11-10 02:04:59,404 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/config.json\n","[INFO|configuration_utils.py:777] 2023-11-10 02:04:59,405 >> Model config T5Config {\n","  \"_name_or_path\": \"sonoisa/t5-base-japanese\",\n","  \"architectures\": [\n","    \"T5Model\"\n","  ],\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"eos_token_ids\": [\n","    1\n","  ],\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_length\": 512,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_beams\": 4,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"transformers_version\": \"4.36.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","Downloading spiece.model: 100% 804k/804k [00:00<00:00, 2.81MB/s]\n","Downloading (…)cial_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 10.4MB/s]\n","[INFO|tokenization_utils_base.py:2022] 2023-11-10 02:05:01,380 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/spiece.model\n","[INFO|tokenization_utils_base.py:2022] 2023-11-10 02:05:01,380 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2022] 2023-11-10 02:05:01,380 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2022] 2023-11-10 02:05:01,380 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2022] 2023-11-10 02:05:01,380 >> loading file tokenizer.json from cache at None\n","[INFO|configuration_utils.py:717] 2023-11-10 02:05:01,380 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/config.json\n","[INFO|configuration_utils.py:777] 2023-11-10 02:05:01,381 >> Model config T5Config {\n","  \"_name_or_path\": \"sonoisa/t5-base-japanese\",\n","  \"architectures\": [\n","    \"T5Model\"\n","  ],\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"eos_token_ids\": [\n","    1\n","  ],\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_length\": 512,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_beams\": 4,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"transformers_version\": \"4.36.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","[WARNING|logging.py:329] 2023-11-10 02:05:01,439 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Downloading pytorch_model.bin: 100% 892M/892M [00:20<00:00, 43.9MB/s]\n","[INFO|modeling_utils.py:3121] 2023-11-10 02:05:23,073 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/pytorch_model.bin\n","[INFO|configuration_utils.py:791] 2023-11-10 02:05:23,590 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"max_length\": 512,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 0\n","}\n","\n","[INFO|modeling_utils.py:3950] 2023-11-10 02:05:25,852 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:3958] 2023-11-10 02:05:25,852 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at sonoisa/t5-base-japanese.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n","[INFO|modeling_utils.py:3525] 2023-11-10 02:05:25,991 >> Generation config file not found, using a generation config created from the model config.\n","Running tokenizer on train dataset:   0% 0/356 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-0e5416dff87e3c72.arrow\n","11/10/2023 02:05:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-0e5416dff87e3c72.arrow\n","Running tokenizer on train dataset: 100% 356/356 [00:02<00:00, 173.71 examples/s]\n","Running tokenizer on validation dataset:   0% 0/89 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-62fcc00c3e4f1f95.arrow\n","11/10/2023 02:05:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-99366a07f8563659/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-62fcc00c3e4f1f95.arrow\n","Running tokenizer on validation dataset: 100% 89/89 [00:00<00:00, 230.93 examples/s]\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 22.9MB/s]\n","[INFO|trainer.py:1724] 2023-11-10 02:05:45,329 >> ***** Running training *****\n","[INFO|trainer.py:1725] 2023-11-10 02:05:45,329 >>   Num examples = 356\n","[INFO|trainer.py:1726] 2023-11-10 02:05:45,329 >>   Num Epochs = 10\n","[INFO|trainer.py:1727] 2023-11-10 02:05:45,330 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1730] 2023-11-10 02:05:45,330 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:1731] 2023-11-10 02:05:45,330 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1732] 2023-11-10 02:05:45,330 >>   Total optimization steps = 890\n","[INFO|trainer.py:1733] 2023-11-10 02:05:45,331 >>   Number of trainable parameters = 222,903,552\n","{'loss': 4.2149, 'learning_rate': 4.438202247191011e-05, 'epoch': 1.12}\n","{'loss': 3.4843, 'learning_rate': 3.876404494382023e-05, 'epoch': 2.25}\n","{'loss': 3.2051, 'learning_rate': 3.314606741573034e-05, 'epoch': 3.37}\n","{'loss': 2.9811, 'learning_rate': 2.752808988764045e-05, 'epoch': 4.49}\n","{'loss': 2.837, 'learning_rate': 2.1910112359550563e-05, 'epoch': 5.62}\n","{'loss': 2.7166, 'learning_rate': 1.6292134831460676e-05, 'epoch': 6.74}\n","{'loss': 2.6701, 'learning_rate': 1.0674157303370787e-05, 'epoch': 7.87}\n","{'loss': 2.6238, 'learning_rate': 5.056179775280899e-06, 'epoch': 8.99}\n","100% 890/890 [05:51<00:00,  2.61it/s][INFO|trainer.py:1956] 2023-11-10 02:11:37,017 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 351.7005, 'train_samples_per_second': 10.122, 'train_steps_per_second': 2.531, 'train_loss': 3.0376483702927493, 'epoch': 10.0}\n","100% 890/890 [05:51<00:00,  2.53it/s]\n","[INFO|trainer.py:2883] 2023-11-10 02:11:37,035 >> Saving model checkpoint to summary_ja/\n","[INFO|configuration_utils.py:461] 2023-11-10 02:11:37,039 >> Configuration saved in summary_ja/config.json\n","[INFO|configuration_utils.py:564] 2023-11-10 02:11:37,042 >> Configuration saved in summary_ja/generation_config.json\n","[INFO|modeling_utils.py:2193] 2023-11-10 02:11:39,113 >> Model weights saved in summary_ja/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2428] 2023-11-10 02:11:39,119 >> tokenizer config file saved in summary_ja/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2437] 2023-11-10 02:11:39,122 >> Special tokens file saved in summary_ja/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2488] 2023-11-10 02:11:39,124 >> added tokens file saved in summary_ja/added_tokens.json\n","***** train metrics *****\n","  epoch                    =       10.0\n","  train_loss               =     3.0376\n","  train_runtime            = 0:05:51.70\n","  train_samples            =        356\n","  train_samples_per_second =     10.122\n","  train_steps_per_second   =      2.531\n","11/10/2023 02:11:41 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:3160] 2023-11-10 02:11:41,640 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3162] 2023-11-10 02:11:41,640 >>   Num examples = 89\n","[INFO|trainer.py:3165] 2023-11-10 02:11:41,640 >>   Batch size = 4\n","[INFO|configuration_utils.py:791] 2023-11-10 02:11:41,650 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"max_length\": 512,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 0\n","}\n","\n","100% 23/23 [00:37<00:00,  1.63s/it]\n","***** eval metrics *****\n","  epoch                   =       10.0\n","  eval_gen_len            =    35.2809\n","  eval_loss               =     3.4724\n","  eval_rouge1             =     1.9476\n","  eval_rouge2             =     0.8427\n","  eval_rougeL             =     1.6479\n","  eval_rougeLsum          =     1.9476\n","  eval_runtime            = 0:00:38.73\n","  eval_samples            =         89\n","  eval_samples_per_second =      2.298\n","  eval_steps_per_second   =      0.594\n","[INFO|modelcard.py:452] 2023-11-10 02:12:20,583 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 1.9476}]}\n","CPU times: user 3.45 s, sys: 500 ms, total: 3.95 s\n","Wall time: 7min 40s\n"]}]}]}